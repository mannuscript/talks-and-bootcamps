{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Tokenization\n",
    "\n",
    "Tokenization splits a text document into, well, tokens, and it is the most basic text processing step. A token is the smallest unit with semantic meaning -- that is, tokens a mostly words an numbers, while letters of words or digits of numbers are not tokens. Punctuation marks are also considered tokens. With the more informal writing style on social media, concepts such as hashtags, emoticons and emojies are nowadays also often meaningful tokens.\n",
    "\n",
    "The following examples compare different tokenizers to highlight the differences and subtleties when it comes to splitting text into tokens. There is not the best or only correct tokenizer. Which implementation is best suitable depends on the type of input and further processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all important packages\n",
    "\n",
    "We first use NLTK, a very popular and mature Python package from language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk import word_tokenize # Simplfied notation; it's a wrapper for the TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides more tokenizer: http://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a document\n",
    "\n",
    "We first create list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Text processing with Python is great.\", \n",
    "             \"It isn't (very) complicated to get started.\",\n",
    "             \"However,careful to...you know....avoid mistakes.\",\n",
    "             \"Contact me at vonderweth@nus.edu.sg; see http://nus.edu.sg.\",\n",
    "             \"This is so cooool #nltkrocks :))) :-P <3.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To form the document, we can use the in-built `join()` method to concatenate all sentences using a whitesplace as seperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text processing with Python is great. It isn't (very) complicated to get started. However,careful to...you know....avoid mistakes. Contact me at vonderweth@nus.edu.sg; see http://nus.edu.sg. This is so cooool #nltkrocks :))) :-P <3.\n"
     ]
    }
   ],
   "source": [
    "document = ' '.join(sentences)\n",
    "\n",
    "# Print the document to see if everything looks alright\n",
    "print (document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document tokenization into sentences\n",
    "\n",
    "Sometime, you just want to split a document into sentences and not individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text processing with Python is great.\n",
      "It isn't (very) complicated to get started.\n",
      "However,careful to...you know....avoid mistakes.\n",
      "Contact me at vonderweth@nus.edu.sg; see http://nus.edu.sg.\n",
      "This is so cooool #nltkrocks :))) :-P <3.\n"
     ]
    }
   ],
   "source": [
    "sentence_tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "# The tokenize() method returns a list containing the sentences\n",
    "sentences_alt = sentence_tokenizer.tokenize(document)\n",
    "\n",
    "# Loop over all sentences and print each sentence\n",
    "for s in sentences_alt:\n",
    "    print (s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document tokenization into tokens\n",
    "\n",
    "In the following, we tokenize each sentence individually. This makes the presentation a bit more convienient. In practice, you can tokenize the whole document at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive tokenization\n",
    "\n",
    "Python provides an in-built method `split()` that splits strings with respect to a user-defined separator. Be default, the separator is a whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output of split() method:\n",
      "['Text', 'processing', 'with', 'Python', 'is', 'great.']\n",
      "['It', \"isn't\", '(very)', 'complicated', 'to', 'get', 'started.']\n",
      "['However,careful', 'to...you', 'know....avoid', 'mistakes.']\n",
      "['Contact', 'me', 'at', 'vonderweth@nus.edu.sg;', 'see', 'http://nus.edu.sg.']\n",
      "['This', 'is', 'so', 'cooool', '#nltkrocks', ':)))', ':-P', '<3.']\n"
     ]
    }
   ],
   "source": [
    "print ('\\nOutput of split() method:')\n",
    "for s in sentences:\n",
    "    print (s.split(' '))\n",
    "    #print(s.split()) # This is also fine since whitespace is the default separator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The limitation of this approach is obivous, since many token are not separated by a whitespace. Most commonly this is the case for punctuation marks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TreebankWordTokenizer\n",
    "\n",
    "The `TreebankWordTokenizer` is the default tokenizer. If you have well-formed text such as news articles, this tokenizer is usually the way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output of TreebankWordTokenizer:\n",
      "['Text', 'processing', 'with', 'Python', 'is', 'great', '.']\n",
      "['It', 'is', \"n't\", '(', 'very', ')', 'complicated', 'to', 'get', 'started', '.']\n",
      "['However', ',', 'careful', 'to', '...', 'you', 'know', '...', '.avoid', 'mistakes', '.']\n",
      "['Contact', 'me', 'at', 'vonderweth', '@', 'nus.edu.sg', ';', 'see', 'http', ':', '//nus.edu.sg', '.']\n",
      "['This', 'is', 'so', 'cooool', '#', 'nltkrocks', ':', ')', ')', ')', ':', '-P', '<', '3', '.']\n",
      "\n",
      "Output of the word_tokenize() method:\n",
      "['Text', 'processing', 'with', 'Python', 'is', 'great', '.']\n",
      "['It', 'is', \"n't\", '(', 'very', ')', 'complicated', 'to', 'get', 'started', '.']\n",
      "['However', ',', 'careful', 'to', '...', 'you', 'know', '...', '.avoid', 'mistakes', '.']\n",
      "['Contact', 'me', 'at', 'vonderweth', '@', 'nus.edu.sg', ';', 'see', 'http', ':', '//nus.edu.sg', '.']\n",
      "['This', 'is', 'so', 'cooool', '#', 'nltkrocks', ':', ')', ')', ')', ':', '-P', '<', '3', '.']\n"
     ]
    }
   ],
   "source": [
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "print ('\\nOutput of TreebankWordTokenizer:')\n",
    "for s in sentences:\n",
    "    print (treebank_tokenizer.tokenize(s))\n",
    "\n",
    "print ('\\nOutput of the word_tokenize() method:')\n",
    "for s in sentences:\n",
    "    print (word_tokenize(s))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both outputs are the same, since the `word_tokenize()` method is just a wrapper for the `TreebankWordTokenizer` to simplify the coding.\n",
    "\n",
    "See how this tokenizer also splits common contractions such as *isn't*, *hasn't*, *haven't*. Other tokenizers (see below) consider such contractions as one token. Being aware how this is handled is, for example, important for sentiment analysis where handling negations is very important to get the sentiment right.\n",
    "\n",
    "Also, notice how the tokenizer can handle the ellipsis (`...`) correctly in the first case but fails in the second case since an ellipsis is by definition comprised of exactly 3 dots. More or less the 3 dots are not handled properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TweetTokenizer\n",
    "\n",
    "The `TweetTokenizer` is optimized for social media content where people use informal concepts such as hashtags or emoticons. Note that emoticons are often contain punctiation marks that throw other tokenizers off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of TweetTokenizer:\n",
      "['Text', 'processing', 'with', 'Python', 'is', 'great', '.']\n",
      "['It', \"isn't\", '(', 'very', ')', 'complicated', 'to', 'get', 'started', '.']\n",
      "['However', ',', 'careful', 'to', '...', 'you', 'know', '...', 'avoid', 'mistakes', '.']\n",
      "['Contact', 'me', 'at', 'vonderweth@nus.edu.sg', ';', 'see', 'http://nus.edu.sg', '.']\n",
      "['This', 'is', 'so', 'cooool', '#nltkrocks', ':)', ')', ')', ':-P', '<3', '.']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "print ('Output of TweetTokenizer:')\n",
    "for s in sentences:\n",
    "    print (tweet_tokenizer.tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, both ellipsis are recognized, with the second one even \"correct\" to three dots.\n",
    "\n",
    "Note how the toknizer fails with `:)))`. The problem is that it is not the \"official version\" of the emoticon -- which is `:)` or `:-)` -- but uses multiple \"mouths\" to emphasize the expressed sentiment of feeling. If this is a problem depends on the subsequent analysis; some extra `)` are no big deal in many cases.\n",
    "\n",
    "The 2 basic alternatives to properly address this issue:\n",
    "- Clean your text before tokenizing\n",
    "- Remove all \"odd\" tokens from the list before further processing\n",
    "- Write your own sophisticated tokenizer :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegexpTokenizer\n",
    "\n",
    "The `RegexpTokenizer` takes as input a regular expression that specifies which parts of the string qualify as a valid token. That means that some parts of the string might be removed. In principle, all previous tokenizers can be expressed as a `RegexpTokenizer` -- however, the required regular expressions can be very complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output of RegexpTokenizer for pattern [a-zA-Z']+:\n",
      "['Text', 'processing', 'with', 'Python', 'is', 'great']\n",
      "['It', \"isn't\", 'very', 'complicated', 'to', 'get', 'started']\n",
      "['However', 'careful', 'to', 'you', 'know', 'avoid', 'mistakes']\n",
      "['Contact', 'me', 'at', 'vonderweth', 'nus', 'edu', 'sg', 'see', 'http', 'nus', 'edu', 'sg']\n",
      "['This', 'is', 'so', 'cooool', 'nltkrocks', 'P']\n"
     ]
    }
   ],
   "source": [
    "pattern = '\\w+' # all alphanumeric words\n",
    "pattern = '[a-zA-Z]+' # all alphanumeric words (without digits)\n",
    "pattern = '[a-zA-Z\\']+' # all alphanumeric words (without digits, but keep contractions)\n",
    "regexp_tokenizer = RegexpTokenizer(pattern)\n",
    "\n",
    "print ('\\nOutput of RegexpTokenizer for pattern {}:'.format(pattern))\n",
    "for s in sentences:\n",
    "    print (regexp_tokenizer.tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization with spaCy\n",
    "\n",
    "spaCy is another Python text processing package. It is rather new but is also very sophisticated and quickly gained a lot of popularity. Its usage is often easier compared to NLTK since spaCy typically combines several processing steps into one, this hiding more of its logic and requiring less lines of codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load English language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't find model 'en'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-76abff010c5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;34m\"to load. For example:\\nnlp = spacy.load('{}')\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             'error')\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exists'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't find model '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't find model 'en'"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process document\n",
    "\n",
    "Compared to NLTK, the common usage of spaCy is to process a string which not only performs tokenization but also other steps (see later tutorial). Here, we only look at the tokens.\n",
    "\n",
    "Again, we process each sentence individually to simplify the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output of spaCy tokenizer:\n",
      "['Text', 'processing', 'with', 'Python', 'is', 'great', '.']\n",
      "['It', 'is', \"n't\", '(', 'very', ')', 'complicated', 'to', 'get', 'started', '.']\n",
      "['However', ',', 'careful', 'to', '...', 'you', 'know', '...', '.avoid', 'mistakes', '.']\n",
      "['Contact', 'me', 'at', 'vonderweth@nus.edu.sg', ';', 'see', 'http://nus.edu.sg', '.']\n",
      "['This', 'is', 'so', 'cooool', '#', 'nltkrocks', ':))', ')', ':-P', '<3', '.']\n"
     ]
    }
   ],
   "source": [
    "print ('\\nOutput of spaCy tokenizer:')\n",
    "for s in sentences:\n",
    "    doc = nlp(s) # doc is an object, not just a simple list\n",
    "    # Let's create a list so the output matches the previous ones\n",
    "    token_list = []\n",
    "    for token in doc:\n",
    "        token_list.append(token.text) # token is also an object, not a string\n",
    "    print (token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy does a bit better with the uncommon emoticon, but splits the hashtag. Also the second ellisis (the one with 4 dots) is not handled correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- There are different tokenizer implementations available, Each with its strengths and weaknesses, and none of them is in all case the best choice.\n",
    "- For formal text, most tokenizers do just fine -- still, it is important how the toknizer works, e.g., how it is handling contractions. A bit more consideration is needed if the input is informal text like it is commonly found on social media.\n",
    "- Since tokenization is usually the first and most basic step, it is worth to \"get it right\" to avoid that any errors or issue get forwarded into subsequent processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
