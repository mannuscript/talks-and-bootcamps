{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Pre-trained Word2vec Solution.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jMJkBtZZq90c","colab_type":"text"},"source":["# 3rd Exercise\n","\n","Prepared by: *Hardian Lawi*"]},{"cell_type":"code","metadata":{"id":"2wo-l_dCwTN8","colab_type":"code","colab":{}},"source":["import re\n","import numpy as np\n","import pandas as pd\n","from gensim.models import KeyedVectors\n","from sklearn.linear_model import LogisticRegression"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZQjqza5wxB4L","colab_type":"code","outputId":"506f1991-5871-456b-d3e8-3b356113b62f","executionInfo":{"status":"ok","timestamp":1562997552677,"user_tz":-420,"elapsed":35715,"user":{"displayName":"Hardian Lawi","photoUrl":"","userId":"05768104717203707125"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["%%bash\n","\n","wget -qO yelp_review_polarity_csv.tgz https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz\n","wget -qO GoogleNews-vectors-negative300.bin.gz \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n","gunzip GoogleNews-vectors-negative300.bin.gz\n","tar -xvzf yelp_review_polarity_csv.tgz\n","ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["yelp_review_polarity_csv/\n","yelp_review_polarity_csv/train.csv\n","yelp_review_polarity_csv/readme.txt\n","yelp_review_polarity_csv/test.csv\n","GoogleNews-vectors-negative300.bin\n","GoogleNews-vectors-negative300.bin.gz\n","sample_data\n","yelp_review_polarity_csv\n","yelp_review_polarity_csv.tgz\n"],"name":"stdout"},{"output_type":"stream","text":["gzip: GoogleNews-vectors-negative300.bin already exists;\tnot overwritten\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"ppnGwoJVipPt","colab_type":"text"},"source":["# Load pre-trained Embeddings"]},{"cell_type":"code","metadata":{"id":"5JjGE09OaMeC","colab_type":"code","outputId":"72e83252-4d58-4866-e896-82113629908e","executionInfo":{"status":"ok","timestamp":1562997700535,"user_tz":-420,"elapsed":183561,"user":{"displayName":"Hardian Lawi","photoUrl":"","userId":"05768104717203707125"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["# Load vectors directly from the file\n","model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"nTT-w-xFir2k","colab_type":"text"},"source":["# Process Dataset"]},{"cell_type":"code","metadata":{"id":"_FxuGTNgw8DX","colab_type":"code","colab":{}},"source":["train = pd.read_csv('yelp_review_polarity_csv/train.csv', names=[\"label\", \"text\"])\n","test = pd.read_csv('yelp_review_polarity_csv/test.csv', names=[\"label\", \"text\"])\n","\n","train = train.head(10000)\n","test = test.head(1000)\n","\n","y_train = train.label - 1\n","y_test = test.label - 1\n","\n","corpus = train.text.to_list() + test.text.to_list()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fjp7iYuHpxpx","colab_type":"text"},"source":["## Tokenization"]},{"cell_type":"code","metadata":{"id":"QU0AEScsjG4V","colab_type":"code","colab":{}},"source":["def tokenize(corpus, lower=True, pattern=r'(?u)\\b\\w\\w+\\b'):\n","  tokenized_corpus = []\n","  rx = re.compile(pattern)\n","  for sent in corpus:\n","    if lower:\n","      tokenized_corpus.append([x.lower() for x in rx.findall(sent)])\n","    else:\n","      tokenized_corpus.append([x for x in rx.findall(sent)])\n","  return tokenized_corpus\n","\n","tokenized_corpus = tokenize(corpus)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HGZ79olcpzxM","colab_type":"text"},"source":["## Convert each sentence to a vector\n","\n"]},{"cell_type":"code","metadata":{"id":"AucOo9b3kzP9","colab_type":"code","colab":{}},"source":["embed_corpus = []\n","invalid_counts = 0\n","for doc in tokenized_corpus:\n","  temp = np.zeros((1, 300), dtype=float)\n","  count = 0\n","  for token in doc:\n","    if token in model.vocab:\n","      temp += model[token]\n","      count += 1\n","  if count != 0:\n","    temp /= count\n","  else:\n","    invalid_counts += 1\n","  embed_corpus.append(temp)\n","embed_corpus = np.concatenate(embed_corpus)\n","\n","print('Invalid counts:', invalid_counts)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"atM3gb69m4Cl","colab_type":"code","colab":{}},"source":["train_emb = embed_corpus[:train.shape[0]]\n","test_emb = embed_corpus[train.shape[0]:]\n","\n","print('train size:', train_emb.shape)\n","print('test size:', test_emb.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n3fyUydHp7-p","colab_type":"text"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"7SQBdj-4nBUR","colab_type":"code","colab":{}},"source":["model = LogisticRegression()\n","model = model.fit(train_emb, y_train)\n","\n","print('Train acc:', (model.predict(train_emb) == y_train).mean())\n","print('Test acc:', (model.predict(test_emb) == y_test).mean())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WdFBAI4moixs","colab_type":"text"},"source":["# Bonus\n","\n","This model performs worse than both TF and TF-idf representation because even though some dimension of the embeddings might be representing the positivity or negativity of the words, by taking the mean embeddings, we are averaging out this effect because in most cases, documents consist of words with neutral sentiment. However, this problem should be mitigated by performing **weighted average** (multiply by TF or TF-idf) of the embeddings. Also, instead of taking the average, we could **concatenate** the embeddings to form a long vector, e.g. each word is represented by 300-dimensional vector, thus, a document of 5 words would be 1500-dimensional vector. However, the latter approach poses another problem, i.e. not all documents are in the same length. Therefore, we could pad the vectors by 0 to form vectors of same length.\n","\n","All the approaches that we have seen do not seem to learn anything regarding the positions of the words. There is a way to incorporate information to our linear model by using *positional embeddings*. Other than this, we could also use model such as *Recurrent Neural Network* that specifically designs to learn from sequence data."]},{"cell_type":"code","metadata":{"id":"IRFAGSkMqPeG","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}