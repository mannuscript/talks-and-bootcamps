{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming & Lemmatization\n",
    "\n",
    "Both stemming and lemmatization are the methods to normalize documents on a syntactical level. Often the same words are used in different forms depending due to grammatical reasons. Consider the following to sentences:\n",
    "\n",
    "- Dogs make the best friends.\n",
    "- A dog makes a good friend.\n",
    "\n",
    "Semantically, both sentences are essentially conveying the same message, but syntactically they are very different since the vocabulary is different: \"dog\" vs. \"dog\", \"make\" vs. \"makes\", \"friends\" vs. \"friend\". This is a big problem when comparing documents or when searching for documents in a database. For example, when one uses \"dog\" as search term, both sentences should be return and not just the second one.\n",
    "\n",
    "While the goals of stemming and lemmatization are similar, there a basic differences: \n",
    "\n",
    " - **Stemmming:** Usually just applying crude heuristics that chop off the end of words. This may result in terms that are no longer proper words.\n",
    " - **Lemmatization:** Using vocabularies and morphological analysis of words to derive the root word for a term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all important packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from utils.nlputil import remove_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 123\n"
     ]
    }
   ],
   "source": [
    "print (remove_punctuation(\"Test, 123.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "We first define a few stemmers provided by NLTK.\n",
    "\n",
    "For more stemmer, see http://www.nltk.org/api/nltk.stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Put all stemmers into a list to make their use easier\n",
    "stemmer_list = [porter_stemmer, snowball_stemmer, lancaster_stemmer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['dogs', 'cats', 'running', 'phones', 'viewed', 'presumably', 'crying', 'went', 'packed', 'worse', 'best', 'mice', 'friends', 'makes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs:\n",
      "\t dog\n",
      "\t dog\n",
      "\t dog\n",
      "cats:\n",
      "\t cat\n",
      "\t cat\n",
      "\t cat\n",
      "running:\n",
      "\t run\n",
      "\t run\n",
      "\t run\n",
      "phones:\n",
      "\t phone\n",
      "\t phone\n",
      "\t phon\n",
      "viewed:\n",
      "\t view\n",
      "\t view\n",
      "\t view\n",
      "presumably:\n",
      "\t presum\n",
      "\t presum\n",
      "\t presum\n",
      "crying:\n",
      "\t cri\n",
      "\t cri\n",
      "\t cry\n",
      "went:\n",
      "\t went\n",
      "\t went\n",
      "\t went\n",
      "packed:\n",
      "\t pack\n",
      "\t pack\n",
      "\t pack\n",
      "worse:\n",
      "\t wors\n",
      "\t wors\n",
      "\t wors\n",
      "best:\n",
      "\t best\n",
      "\t best\n",
      "\t best\n",
      "mice:\n",
      "\t mice\n",
      "\t mice\n",
      "\t mic\n",
      "friends:\n",
      "\t friend\n",
      "\t friend\n",
      "\t friend\n",
      "makes:\n",
      "\t make\n",
      "\t make\n",
      "\t mak\n"
     ]
    }
   ],
   "source": [
    "for word in word_list:\n",
    "    print (word + ':')\n",
    "    for stemmer in stemmer_list:\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        print ('\\t', stemmed_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "The output of a lemmatizer, in general, depends on the type of word (noun, verb, or adjective). For example, when used as an adjective \"running\" (e.g., \"a running tap\") the word is already in its base form. However, \"running\" used as a verb (e.g., \"he was running away\") then the base form is \"run\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs:\n",
      "\t dogs =[n]=> dog\n",
      "\t dogs =[v]=> dog\n",
      "\t dogs =[a]=> dogs\n",
      "cats:\n",
      "\t cats =[n]=> cat\n",
      "\t cats =[v]=> cat\n",
      "\t cats =[a]=> cats\n",
      "running:\n",
      "\t running =[n]=> running\n",
      "\t running =[v]=> run\n",
      "\t running =[a]=> running\n",
      "phones:\n",
      "\t phones =[n]=> phone\n",
      "\t phones =[v]=> phone\n",
      "\t phones =[a]=> phones\n",
      "viewed:\n",
      "\t viewed =[n]=> viewed\n",
      "\t viewed =[v]=> view\n",
      "\t viewed =[a]=> viewed\n",
      "presumably:\n",
      "\t presumably =[n]=> presumably\n",
      "\t presumably =[v]=> presumably\n",
      "\t presumably =[a]=> presumably\n",
      "crying:\n",
      "\t crying =[n]=> cry\n",
      "\t crying =[v]=> cry\n",
      "\t crying =[a]=> crying\n",
      "went:\n",
      "\t went =[n]=> went\n",
      "\t went =[v]=> go\n",
      "\t went =[a]=> went\n",
      "packed:\n",
      "\t packed =[n]=> packed\n",
      "\t packed =[v]=> pack\n",
      "\t packed =[a]=> packed\n",
      "worse:\n",
      "\t worse =[n]=> worse\n",
      "\t worse =[v]=> worse\n",
      "\t worse =[a]=> bad\n",
      "best:\n",
      "\t best =[n]=> best\n",
      "\t best =[v]=> best\n",
      "\t best =[a]=> best\n",
      "mice:\n",
      "\t mice =[n]=> mouse\n",
      "\t mice =[v]=> mice\n",
      "\t mice =[a]=> mice\n",
      "friends:\n",
      "\t friends =[n]=> friend\n",
      "\t friends =[v]=> friends\n",
      "\t friends =[a]=> friends\n",
      "makes:\n",
      "\t makes =[n]=> make\n",
      "\t makes =[v]=> make\n",
      "\t makes =[a]=> makes\n"
     ]
    }
   ],
   "source": [
    "word_type_list = ['n', 'v', 'a']\n",
    "\n",
    "for word in word_list:\n",
    "    print (word + ':')\n",
    "    for word_type in word_type_list:\n",
    "        lemmatized_word = wordnet_lemmatizer.lemmatize(word, pos=word_type) # default is 'n'\n",
    "        print ('\\t', word, '=[{}]=>'.format(word_type), lemmatized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show a complete example, we already look ahead and use a Part-of-Speech (POS) tagger that tells use the type for each word in sentence (see the follow-up tutorial for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The newest study has shown that cats have a better sense of smell than dogs.\"\n",
    "#sentence = \"Dogs make the best friends.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('newest', 'JJS'), ('study', 'NN'), ('has', 'VBZ'), ('shown', 'VBN'), ('that', 'IN'), ('cats', 'NNS'), ('have', 'VBP'), ('a', 'DT'), ('better', 'JJR'), ('sense', 'NN'), ('of', 'IN'), ('smell', 'NN'), ('than', 'IN'), ('dogs', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# First, tokenize sentence\n",
    "token_list = word_tokenize(sentence)\n",
    "\n",
    "# Second, calculate POS tags for each token\n",
    "pos_tag_list = pos_tag(token_list)\n",
    "\n",
    "print (pos_tag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The POS tagger distinguishes several dozens of word types. However, we are only interested wether a word is a noun, verb, or adjective. We therefore need to map the output of the POS tagger to the 3 valid options \"n\", \"v\", and \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output of NLTK lemmatizer:\n",
      "\n",
      "The =[DT]==[n]=> the\n",
      "newest =[JJS]==[a]=> new\n",
      "study =[NN]==[n]=> study\n",
      "has =[VBZ]==[v]=> have\n",
      "shown =[VBN]==[v]=> show\n",
      "that =[IN]==[n]=> that\n",
      "cats =[NNS]==[n]=> cat\n",
      "have =[VBP]==[v]=> have\n",
      "a =[DT]==[n]=> a\n",
      "better =[JJR]==[a]=> good\n",
      "sense =[NN]==[n]=> sense\n",
      "of =[IN]==[n]=> of\n",
      "smell =[NN]==[n]=> smell\n",
      "than =[IN]==[n]=> than\n",
      "dogs =[NNS]==[n]=> dog\n",
      ". =[.]==[n]=> .\n"
     ]
    }
   ],
   "source": [
    "print ('\\nOutput of NLTK lemmatizer:\\n')\n",
    "for token, tag in pos_tag_list:\n",
    "    word_type = 'n'\n",
    "    tag_simple = tag[0].lower() # Converts, e.g., \"VBD\" to \"c\"\n",
    "    if tag_simple in ['n', 'v']:\n",
    "        # If the POS tag starts with \"n\" or \"v\", we know it's a noun or verb\n",
    "        word_type = tag_simple \n",
    "    elif tag_simple in ['j']:\n",
    "        # If the POS tag starts with a \"j\", we know it's an adjective\n",
    "        word_type = 'a' \n",
    "    lemmatized_token = wordnet_lemmatizer.lemmatize(token.lower(), pos=word_type)\n",
    "    print(token, '=[{}]==[{}]=>'.format(tag, word_type), lemmatized_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy already performs lemmatization by default when processing a document without any additional commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output of spaCy lemmatizer:\n",
      "The =DET=> the\n",
      "newest =ADJ=> new\n",
      "study =NOUN=> study\n",
      "has =VERB=> have\n",
      "shown =VERB=> show\n",
      "that =ADP=> that\n",
      "cats =NOUN=> cat\n",
      "have =VERB=> have\n",
      "a =DET=> a\n",
      "better =ADJ=> good\n",
      "sense =NOUN=> sense\n",
      "of =ADP=> of\n",
      "smell =NOUN=> smell\n",
      "than =ADP=> than\n",
      "dogs =NOUN=> dog\n",
      ". =PUNCT=> .\n"
     ]
    }
   ],
   "source": [
    "print ('\\nOutput of spaCy lemmatizer:')\n",
    "doc = nlp(sentence) # doc is an object, not just a simple list\n",
    "# Let's create a list so the output matches the previous ones\n",
    "token_list = []\n",
    "for token in doc:\n",
    "    print (token.text, '={}=>'.format(token.pos_), token.lemma_) # token is also an object, not a string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the spaCy lemmatizer, compared to the NLTK lemmatizer, does not convert \"better\" to \"good\" although correctly identified as adjective. On the other hand, \"newest\" gets converted to \"new\". The spaCy lemmatizer also converts all tokens/word to lowercase, which is typically does not matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application use case: document similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two methods take a document as input and return a set of words (i.e., no duplicates). `create_stemmed_word_set()` stems each word; `create_lemmatized_word_set()` lemmatizes each word. The methods simply put together all the individial steps as previously shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.nlputil import preprocess_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some example output for both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newest studi ha shown cat better sens smell dog\n",
      "new study show cat good sense smell dog\n"
     ]
    }
   ],
   "source": [
    "# Show example output of create_stemmed_word_set() method\n",
    "print (preprocess_text(sentence, stemmer=porter_stemmer))\n",
    "\n",
    "# Show example output of create_lemmatized_word_set() method\n",
    "print (preprocess_text(sentence, lemmatizer=wordnet_lemmatizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To caluclate the similarity between two documents, let's define a second sentence that is sematically similar to the first one, but not syntactically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = \"The newest study has shown that cats have a better sense of smell than dogs.\"\n",
    "sentence_2 = \"Some studies show that a cat can smell better than a dog.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both sentences, we can caluculate all 3 different word sets:\n",
    "- naive (only simple tokenizing)\n",
    "- stemmed\n",
    "- lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shown', 'cats', 'study', '.', 'sense', 'the', 'than', 'smell', 'better', 'have', 'that', 'newest', 'has', 'dogs', 'a', 'of'}\n",
      "{'dog', 'shown', 'sens', 'cat', 'smell', 'better', 'ha', 'newest', 'studi'}\n",
      "{'dog', 'good', 'study', 'cat', 'sense', 'smell', 'new', 'show'}\n"
     ]
    }
   ],
   "source": [
    "naive_word_set_1 = set(word_tokenize(sentence.lower()))\n",
    "naive_word_set_2 = set(word_tokenize(sentence_2.lower()))\n",
    "\n",
    "stemmed_word_set_1 = preprocess_text(sentence, stemmer=porter_stemmer, return_type='set')\n",
    "stemmed_word_set_2 = preprocess_text(sentence_2, stemmer=porter_stemmer, return_type='set')\n",
    "\n",
    "lemmatized_word_set_1 = preprocess_text(sentence, lemmatizer=wordnet_lemmatizer, return_type='set')\n",
    "lemmatized_word_set_2 = preprocess_text(sentence_2, lemmatizer=wordnet_lemmatizer, return_type='set')\n",
    "\n",
    "print (naive_word_set_1)\n",
    "print (stemmed_word_set_1)\n",
    "print (lemmatized_word_set_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(word_set_1, word_set_2):\n",
    "    union_set = word_set_1.union(word_set_2)\n",
    "    intersection_set = word_set_1.intersection(word_set_2)\n",
    "    similarity = len(intersection_set) / len(union_set)\n",
    "    return similarity\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To qunatify the similarity between two word sets A and B, we can use the *Jaccard Similarity* J(A,B) as defined as:\n",
    "\n",
    "$$J(A,B)=\\frac{|A\\cap B|}{|A\\cup B|}$$\n",
    "\n",
    "Inuitively, if A and B are completely different, the size interesection $|A\\cap B|$ is 0, making the similarity 0. If A and B are identical both the size intersection and the size of the union are the same, making the similarity 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2727272727272727\n",
      "0.5\n",
      "0.75\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "print (jaccard_similarity(naive_word_set_1, naive_word_set_2))\n",
    "print (jaccard_similarity(stemmed_word_set_2, stemmed_word_set_1))\n",
    "print (jaccard_similarity(lemmatized_word_set_1, lemmatized_word_set_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
