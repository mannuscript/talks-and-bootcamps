{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a simple corpus of 4 documents, each containg just one sentence. All sentences have been preprocessed -- that is, all words a lower case, punctuation has been removed, and stop words have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = ['cat climb tree',\n",
    "             'saw cat climb shed',\n",
    "             'have tree shed shed',\n",
    "             'dog saw cat tree shed']\n",
    "\n",
    "# Remember the number of documents for later\n",
    "N = len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first analyse all documents and build up a series of statistics we need through the tutorial:\n",
    "\n",
    "- `word_list`: a list of all words, i.e., the vocabulary of the corpus\n",
    "\n",
    "- `word_to_idx`: a dictionary that maps a word to an index; most algorithms don't consider words but indexes/numbers for efficiency reasons. Don't forget, algorithms don't care about words specifically. Note below that the index matches the position in the `word_list`. As such, we can not only map from a word to the index but also from an index to the respective word.\n",
    "\n",
    "- `doc_counts`: a dictionary that keeps track in how many documents contain a certain word. This later simplifies the calculation of the inverse document frequency (idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tree': 2, 'climb': 1, 'saw': 3, 'have': 5, 'shed': 4, 'cat': 0, 'dog': 6}\n",
      "['cat', 'climb', 'tree', 'saw', 'shed', 'have', 'dog']\n",
      "{'tree': 3, 'climb': 2, 'saw': 2, 'have': 1, 'shed': 3, 'cat': 3, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "word_to_idx = {}\n",
    "doc_counts = {}\n",
    "\n",
    "for doc in documents:\n",
    "    # For each word in the document...\n",
    "    for word in doc.split():\n",
    "        # If we haven't seen this word yet...\n",
    "        if word not in word_to_idx:\n",
    "            # Add word to word list\n",
    "            word_list.append(word)\n",
    "            # Add word to index mapping; the index is just an increasing number\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "    # for each UNIQUE word in the document...\n",
    "    for word in set(doc.split()):\n",
    "        if word not in doc_counts:\n",
    "            doc_counts[word] = 1\n",
    "        else:\n",
    "            doc_counts[word] += 1\n",
    "            \n",
    "# Let's see how the 3 statistics look like            \n",
    "print(word_to_idx)\n",
    "print(word_list)\n",
    "print(doc_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Word Matrix \n",
    "\n",
    "The most commen way to represent a corpus is the document word matrix, which is simply a more self-explanatory name for a feature set. The matrix contains N rows (N = number of documents) and M columns (M = size of vocabulary). A row is also called a *document vector* or *feature vector*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-count document word matrix (manually)\n",
    "\n",
    "Here, an element indicates how often a word is contained in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 0, 0, 0]\n",
      "[1, 1, 0, 1, 1, 0, 0]\n",
      "[0, 0, 1, 0, 2, 1, 0]\n",
      "[1, 0, 1, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    feature_vector = [0] * len(word_list)  # We know that the length of the vector is the size of the vocabulary\n",
    "    for w in doc.split():\n",
    "        # We use the index mapping to find the correct index of a word.\n",
    "        feature_vector[word_to_idx[w]] += 1\n",
    "            \n",
    "    print(feature_vector) # Each feature vector represents one document; let's see it\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 4 list are the for feature vectors and form together the document term matrix. The order of the columns matches order of the words in the vocabulary with respect to their index. For example, the first word (idx=0) is \"cat\", the second words (idx=1) is \"climb\", and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-count document word matrix (using scikit-learn)\n",
    "\n",
    "`scikit-learn` provides the `CountVectorizer` to easily generate the document word matrix, with the word counts as its elements. `CountVectorizer` support a lot of parameters for configuration -- as we see later. Right now, we simply use the default parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `fit_transform()` takes the corpus as input and performs the generation of the document word matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'climb', 'dog', 'have', 'saw', 'shed', 'tree']\n"
     ]
    }
   ],
   "source": [
    "tf = count_vectorizer.fit_transform(documents)\n",
    "vocabulary = count_vectorizer.get_feature_names()\n",
    "\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `pandas` to conveniently display the document word matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cat  climb  dog  have  saw  shed  tree\n",
      "0    1      1    0     0    0     0     1\n",
      "1    1      1    0     0    1     1     0\n",
      "2    0      0    0     1    0     2     1\n",
      "3    1      0    1     0    1     1     1\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "print(DataFrame(tf.A, columns=vocabulary).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the order of the columsn not line up perfectly. For example, here the last columns is for the word \"tree\", while above the last columns is for word \"dog\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf document word matrix (manually)\n",
    "\n",
    "Here, an element represent the tf-idf value of a word in a document (tf = term frequency; idf = inverse document frequency). The `tf-idf` value of a term $t_i$ in a document $d_j$ is defined as:\n",
    "\n",
    "$$tfidf(t_i, d_j) = tf(t_i, d_j) \\cdot idf(t_i, d_j)$$\n",
    "\n",
    "with\n",
    "\n",
    "$$ tf(t_i, d_j) = \\frac{number\\ of\\ times\\ t_i\\ appears\\ in\\ d_j}{total\\ number\\ of\\ terms\\ in\\ d_j} $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ idf(t_i, d_j) = \\log{\\frac{total\\ number\\ of\\ documents}{number\\ of\\ terms\\ containing\\ t_i}} $$\n",
    "\n",
    "\n",
    "**IMPORTANT**: In the following, we calculate the term frequncy simply as $tf(t_i, d_j) = number\\ of\\ times\\ t_i\\ appears\\ in\\ d_j$. This is how `scikit-learn` does it, and we want to end up with the same result for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.223144, 1.510826, 1.223144, 0.0, 0.0, 0.0, 0.0]\n",
      "[1.223144, 1.510826, 0.0, 1.510826, 1.223144, 0.0, 0.0]\n",
      "[0.0, 0.0, 1.223144, 0.0, 2.446287, 1.916291, 0.0]\n",
      "[1.223144, 0.0, 1.223144, 1.510826, 1.223144, 0.0, 1.916291]\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    feature_vector = [0.0] * len(word_list)  # We know that the length of the vector is the size of the vocabulary\n",
    "    # Split the document into list of words\n",
    "    words = doc.split()\n",
    "    # Generate a set of words from the list (i.e., no duplicates)\n",
    "    word_set = set(words)\n",
    "    # For each unique word in the document (i.e., the word set)\n",
    "    for word in word_set:\n",
    "        # Calculate tf; count() is an in-built method the returns the number of occurrences of an item in a list\n",
    "        tf = words.count(word) # / len(words)   # Here we differ from the \"official definition\" of tf\n",
    "        # Calculate the document frequence df which we already did when building the statistics\n",
    "        df = doc_counts[word]\n",
    "        # Calculate the idf; the +1 are for smoothing to match the results of the scikit-learn methods\n",
    "        idf = math.log( (N + 1) / (df + 1) ) + 1\n",
    "        # Finally, caluclate the tf-idf value\n",
    "        tfidf = tf * idf\n",
    "        feature_vector[word_to_idx[word]] = round(tfidf, 6)\n",
    "\n",
    "    print(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf document word matrix (using scikit-learn)\n",
    "\n",
    "Again, `scikit-learn` provides means to hide all these calculations; here, the `TfidfVectorizer`. Be default, `TfidfVectorizer` normalizes the final matrix so that all values are betweent 0 and 1. To get comparable results, we switch this of with `norm=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(norm=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `fit_transform()` takes the corpus as input and performs the generation of the document word matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'climb', 'dog', 'have', 'saw', 'shed', 'tree']\n"
     ]
    }
   ],
   "source": [
    "tfidf_model = tfidf_vectorizer.fit_transform(documents)\n",
    "vocabulary = tfidf_vectorizer.get_feature_names()\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat     climb       dog      have       saw      shed      tree\n",
      "0  1.223144  1.510826  0.000000  0.000000  0.000000  0.000000  1.223144\n",
      "1  1.223144  1.510826  0.000000  0.000000  1.510826  1.223144  0.000000\n",
      "2  0.000000  0.000000  0.000000  1.916291  0.000000  2.446287  1.223144\n",
      "3  1.223144  0.000000  1.916291  0.000000  1.510826  1.223144  1.223144\n"
     ]
    }
   ],
   "source": [
    "print(DataFrame(tfidf_model.A, columns=vocabulary).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All vectorizer allow users to specify a wide range of paramters. A very important one is `ngram_range` which allows to specify to not only use words/tokens (1-grams, unigrams) but also n-gram of larger sizes. Note that n-grams larger than 3 are typically not recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'cat climb', 'cat tree', 'climb', 'climb shed', 'climb tree', 'dog', 'dog saw', 'have', 'have tree', 'saw', 'saw cat', 'shed', 'shed shed', 'tree', 'tree shed']\n"
     ]
    }
   ],
   "source": [
    "ngram_count_vectorizer = CountVectorizer(ngram_range=(1,2))  # Default is ngram_range=(1,1), i.e., individual words\n",
    "\n",
    "ngram_count_model = ngram_count_vectorizer.fit_transform(documents)\n",
    "ngram_vocabulary = ngram_count_vectorizer.get_feature_names()\n",
    "\n",
    "print(ngram_vocabulary)\n",
    "#print(DataFrame(ngram_count_model.A, columns=ngram_vocabulary).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cat  cat climb  cat tree  climb  climb shed  climb tree  dog  dog saw  have  have tree  saw  saw cat  shed  shed shed  tree  tree shed\n",
      "0    1          1         0      1           0           1    0        0     0          0    0        0     0          0     1          0\n",
      "1    1          1         0      1           1           0    0        0     0          0    1        1     1          0     0          0\n",
      "2    0          0         0      0           0           0    0        0     1          1    0        0     2          1     1          1\n",
      "3    1          0         1      0           0           0    1        1     0          0    1        1     1          0     1          1\n"
     ]
    }
   ],
   "source": [
    "print(DataFrame(ngram_count_model.A, columns=ngram_vocabulary).to_string())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
