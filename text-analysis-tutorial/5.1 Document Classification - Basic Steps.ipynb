{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification - Basic Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all important packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold, cross_val_score # import KFold\n",
    "\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "### Read data files\n",
    "\n",
    "Use Python package `pandas` to read files. This dataset consists of 2 text files, one containing 5,331 positive sentences and the other 5,331 negative sentiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  the rock is destined to be the 21st century's ...\n",
       "1  the gorgeously elaborate continuation of \" the...\n",
       "2                     effective but too-tepid biopic\n",
       "3  if you sometimes like to go to the movies to h...\n",
       "4  emerges as something rare , an issue movie tha..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent_pos = pd.read_csv('data/sentence-polarity-dataset/sentence-polarity.pos', sep='\\t', header=None)\n",
    "df_sent_neg = pd.read_csv('data/sentence-polarity-dataset/sentence-polarity.neg', sep='\\t', header=None)\n",
    "\n",
    "df_sent_pos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create internal representation of dataset\n",
    "\n",
    "For the training and testing, we want two lists, one containing all sentences and another containing the respective labels (here `0` representing negative ans `1` representing positive sentences). Note that there is nothing special about labelling the classes. We could equally use the strings `\"negative\"` and `\"positive\"`. Some additional explanations:\n",
    "\n",
    "- The list method `A.extend(B)` attaches list `B` to list `A`\n",
    "\n",
    "- `[0]*len(df_sent_neg)` creates a a list `[0, 0, 0, 0, 0, ...]` of length $N$ with $N$ being the number of, here, negative sentences\n",
    "\n",
    "- `np.array(A)` converts a normal n-dimensional Python list into an n-dimensional numpy array (see `import numpy as np` above). It is not crucial since methods for training and test take both standard lists and numpy arrays as input, but numpy arrays come with a long list of useful functions and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list for all sentences and ad the sentences from both read files\n",
    "sentences = []\n",
    "sentences.extend(df_sent_neg[0].tolist())\n",
    "sentences.extend(df_sent_pos[0].tolist())\n",
    "\n",
    "# Create a list for all lables\n",
    "polarities = []\n",
    "polarities.extend([0]*len(df_sent_neg))\n",
    "polarities.extend([1]*len(df_sent_pos))\n",
    "\n",
    "# Convert from lists to numpy arrays\n",
    "sentences = np.array(sentences)\n",
    "polarities = np.array(polarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, the dataset contains 5,331 positive sentences followed by 5,331 negative sentence. Before we can split the dataset into training and test data, we first have to shuffle the order to ensure a balanced dataset to in turn ensure a balanced training and test data size. Some additional explanations:\n",
    "\n",
    "- `combined = list(zip(sentences, polarities))`: We have to lists containing the sentences and the labels. Of course, we have to ensure the both list are shuffled the same way so that each label keeps associated with the same sentence. The `zip()` method accomplishes this, both zipping and unzipping.\n",
    "\n",
    "- `random.seed(int)` (optional): the `shuffle()` method does not truly randomize the order of the elements of a list, but generates a \"pseudo-randomized\" order. This in turn allows that, by providing a fixed $seed$, we can ensure that shuffling always returns the same \"random\" order. This makes the whole process deterministic and can be useful find problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = list(zip(sentences, polarities))\n",
    "\n",
    "#random.seed(1) (optional)\n",
    "random.shuffle(combined)\n",
    "\n",
    "# split the \"zipped\" list into the two lists of sentences and labels/polarities\n",
    "sentences[:], polarities[:] = zip(*combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training and test data\n",
    "\n",
    "Given 100% of the data, a common way is to split it into 80% (90%) of training data and 20% (10%) of test data. The training is only done using the training and the test only using the testing data, respectively. To make meaningful statements about the effectiveness of the classifiers requires (at least) the the testing is done using data the classifiers has never seen before. Some additional explanations:\n",
    "\n",
    "- `A[:n]` returns the first $n$ elements of list A\n",
    "\n",
    "- `A[n:]` returns all the elements after the $n$-th elemnts of list A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 8529\n",
      "Size of test: 2133\n"
     ]
    }
   ],
   "source": [
    "# Let's go for a 80%/20% split -- you can change the value anf see its effects\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "# Calculate the size of the training data (the size of the dest data is also implicitly given)\n",
    "train_set_size = int(train_test_ratio * len(sentences))\n",
    "\n",
    "# Split data and labels into training and test data with respect to the size of the test data\n",
    "X_train, X_test = sentences[:train_set_size], sentences[train_set_size:]\n",
    "y_train, y_test = polarities[:train_set_size], polarities[train_set_size:]\n",
    "\n",
    "print(\"Size of training set: {}\".format(len(X_train)))\n",
    "print(\"Size of test: {}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate feature set\n",
    "\n",
    "The feature set is the document n-gram matrix with n-grams of size 1 and 3. You can change this value, e.g., `ngram_range=(1, 1)` to consider only unigrams (i.e, individual words/tokens) or `ngram_range=(1, 2)` to consider only unigrams and bigrams. Larger values are the less common since the size of the feature vectors quickly explodes.\n",
    "\n",
    "First, we define the `TfidfVectorizer` give out specification..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and then generate feature set as the document n-gram matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier\n",
    "\n",
    "We use the Multinomial Naive Bayes (MultinomialNB) classifier which usually provides good results from word/n-gram features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_classifier = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test classifier\n",
    "\n",
    "We first need to generate the document n-gram matrix from the test data given the vocabulary of the vecotrizer derived from the training data: `transform()` instead of `fit_transform()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  can now use the trained classifier to predict the polarities for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mnb_classifier.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`classification_report()` is a useful method to quickly show the results of the evaluation by means of precision, recall, and f1-score for each class (here only to) as well as the average precision, recall, and f1-score. By default, the average is a weighted average. The weight is the support, i.e., the number of data items for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.82      0.80      1074\n",
      "          1       0.81      0.77      0.79      1059\n",
      "\n",
      "avg / total       0.79      0.79      0.79      2133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using k-fold cross validation\n",
    "\n",
    "K-fold cross validation splits a dataset into $k$ equally sized blocks and performs $k$ training-testing cycles using always $k-1$ different blocks for training the remaining block for testing.\n",
    "\n",
    "- `cross_val_score()` is a handy method to automize the k-fold cross validation\n",
    "\n",
    "Note that in this example we use only the training data (`X_train_tfidf` and `y_train`) for the cross validation which are is only 80% of the whole dataset. This adheres to the notion to use the training data and validation data for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7957497  0.7719715  0.79627474 0.77180406 0.77974087 0.7688734\n",
      " 0.76407186 0.78271309 0.77584204 0.78708134]\n"
     ]
    }
   ],
   "source": [
    "f1_scores_list = cross_val_score(MultinomialNB(), X_train_tfidf, y_train, cv=10, scoring ='f1')\n",
    "\n",
    "print(f1_scores_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, the reported result are the average scores and their standard variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score (mean/average): 0.779\n",
      "f1 score (standard deviation): 0.010\n"
     ]
    }
   ],
   "source": [
    "print(\"f1 score (mean/average): {:.3f}\".format(f1_scores_list.mean()))\n",
    "print(\"f1 score (standard deviation): {:.3f}\".format(f1_scores_list.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A low standard deviation for the results is a good sign. If some score differ too much from the rest usually indicates that (a) the dataset is not well shuffled to always ensure a balanced training and test dataset or that (b) that the size of the dataset is simply not large enough to properly learn in all cases.\n",
    "\n",
    "For the sake of completeness, the block below performs the k-fold cross validation over the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score (mean/average): 0.783\n",
      "f1 score (standard deviation): 0.013\n"
     ]
    }
   ],
   "source": [
    "# Convert all sentences (100% of the dataset) into the feature set\n",
    "sentences_tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Perform 10-fold cross validation over all sentences\n",
    "f1_scores_list = cross_val_score(MultinomialNB(), sentences_tfidf, polarities, cv=10, scoring ='f1')\n",
    "\n",
    "# Print reported numbers\n",
    "print(\"f1 score (mean/average): {:.3f}\".format(f1_scores_list.mean()))\n",
    "print(\"f1 score (standard deviation): {:.3f}\".format(f1_scores_list.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results should be a little bit better since we simply use more data for the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A full and proper example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LinearSVC, n-gram size: 1 ==> f1-score: 0.770 [0:00:00.977359]\n",
      "Classifier: LinearSVC, n-gram size: 2 ==> f1-score: 0.777 [0:00:01.729566]\n",
      "Classifier: LinearSVC, n-gram size: 3 ==> f1-score: 0.772 [0:00:02.716950]\n",
      "Classifier: LinearSVC, n-gram size: 4 ==> f1-score: 0.770 [0:00:03.818618]\n",
      "Classifier: MultinomialNB, n-gram size: 1 ==> f1-score: 0.777 [0:00:00.466956]\n",
      "Classifier: MultinomialNB, n-gram size: 2 ==> f1-score: 0.782 [0:00:01.180710]\n",
      "Classifier: MultinomialNB, n-gram size: 3 ==> f1-score: 0.779 [0:00:02.177325]\n",
      "Classifier: MultinomialNB, n-gram size: 4 ==> f1-score: 0.778 [0:00:03.262065]\n",
      "Classifier: DecisionTreeClassifier, n-gram size: 1 ==> f1-score: 0.611 [0:00:30.683293]\n",
      "Classifier: DecisionTreeClassifier, n-gram size: 2 ==> f1-score: 0.615 [0:01:42.058731]\n",
      "Classifier: DecisionTreeClassifier, n-gram size: 3 ==> f1-score: 0.608 [0:03:09.032940]\n",
      "Classifier: DecisionTreeClassifier, n-gram size: 4 ==> f1-score: 0.599 [0:04:56.483723]\n",
      "\n",
      "Best f1-score: 0.782 [classifier: MultinomialNB,n-gram size: 2]\n"
     ]
    }
   ],
   "source": [
    "best_score = -1.0\n",
    "best_classifier = None\n",
    "best_ngram_size = -1\n",
    "\n",
    "ngram_sizes = [1, 2, 3, 4]\n",
    "classifiers = [LinearSVC(), MultinomialNB(), DecisionTreeClassifier()]\n",
    "#classifiers = [LinearSVC(), MultinomialNB() ]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    for s in ngram_sizes:\n",
    "        # Save the start time\n",
    "        start_time = datetime.now() \n",
    "        # Define vecotrizer to generate feature set as document ngram matrix\n",
    "        tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, s))\n",
    "        # Generate feature set as document ngram matrix\n",
    "        X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "        # Perform 10-fold valdiation only over training data\n",
    "        f1_scores_list = cross_val_score(classifier, X_train_tfidf, y_train, cv=10, scoring ='f1')\n",
    "        # Calculate the average core\n",
    "        average_f1_score = f1_scores_list.mean()\n",
    "        # Caluclate the required runtime for this parameter setting:\n",
    "        time_elapsed = datetime.now() - start_time\n",
    "        # Print results for current setting\n",
    "        print(\"Classifier: {}, n-gram size: {} ==> f1-score: {:.3f} [{}]\".format(type(classifier).__name__, s, average_f1_score, time_elapsed))\n",
    "        # If the average score is better than the current best score, save all the current parameter values\n",
    "        if average_f1_score > best_score:\n",
    "            best_score = average_f1_score\n",
    "            best_ngram_size = s\n",
    "            best_classifier = classifier\n",
    "\n",
    "print()\n",
    "print(\"Best f1-score: {:.3f} [classifier: {},n-gram size: {}]\".format(best_score, type(best_classifier).__name__, best_ngram_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two important things to notice:\n",
    "\n",
    "- More features do not automatically yield better results\n",
    "\n",
    "- More features usually also result in larger training times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with the best parameter setting(s)\n",
    "\n",
    "Having identified the best parameter settings (i.e., which classifier and which n-gram size), we can train a classifier over the whole training data. Note that this the one and only time we touch the test data `X_test`. The k-fold cross validation was done using `X_train` only, which splits it into the actual training set and the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.82      0.80      1074\n",
      "          1       0.81      0.78      0.79      1059\n",
      "\n",
      "avg / total       0.80      0.80      0.80      2133\n",
      "\n",
      "Accuracy: 0.797\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, best_ngram_size))\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "best_classifier = best_classifier.fit(X_train_tfidf, y_train)\n",
    "y_pred = best_classifier.predict(X_test_tfidf)\n",
    "\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the final results are a bit better then the ones from the cross validation since the training has been done over the whole training data. These numbers are finally the ones that are usually reported. That the averafe f1-score and the accuracy is almost identical is not a given, but is the result of a almost perfectly balanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of results\n",
    "\n",
    "### A random classifier (flipping a fair coin - 2 classes!) and a stupid classifier (always says \"positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_random(y):\n",
    "    y_pred_random = []\n",
    "    for _ in range(len(y)):\n",
    "        y_pred_random.append(random.randint(0, 1))\n",
    "    return y_pred_random\n",
    "    \n",
    "def predict_stupid(y):\n",
    "    y_pred_stupid = []\n",
    "    for _ in range(len(y_test)):\n",
    "        y_pred_stupid.append(1)\n",
    "    return y_pred_stupid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for random classifier:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.48      0.48      1045\n",
      "          1       0.50      0.49      0.49      1088\n",
      "\n",
      "avg / total       0.49      0.49      0.49      2133\n",
      "\n",
      "Accuracy: 0.488\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Results for stupid classifier:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      1045\n",
      "          1       0.51      1.00      0.68      1088\n",
      "\n",
      "avg / total       0.26      0.51      0.34      2133\n",
      "\n",
      "Accuracy: 0.510\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred_random = predict_random(y_test)\n",
    "y_pred_stupid = predict_stupid(y_test)\n",
    "    \n",
    "print(\"Results for random classifier:\")\n",
    "print(classification_report(y_test, y_pred_random))\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, y_pred_random)))\n",
    "print()\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print()\n",
    "print(\"Results for stupid classifier:\")\n",
    "print(classification_report(y_test, y_pred_stupid))\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, y_pred_stupid)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of skewed dataset\n",
    "\n",
    "The following steps generate an imbalance dataset. Since we do not reall train anything here, we only need to do this for the labels, and the order does not matter at all. So the final list of labels contains, e.g., 90% 1's and 10% 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a ratio how much the labels are skewed towards \"positive\"\n",
    "skew_ratio = 0.9\n",
    "\n",
    "pos_set_size = int(skew_ratio * len(y_test))\n",
    "\n",
    "y_test_pos = [1] * pos_set_size\n",
    "y_test_neg = [0] * (len(y_test) - pos_set_size)\n",
    "\n",
    "y_test_skewed = np.concatenate((y_test_pos, y_test_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the random and stupid classifier over the skewed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for random classifier:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.11      0.54      0.18       214\n",
      "          1       0.91      0.50      0.65      1919\n",
      "\n",
      "avg / total       0.83      0.51      0.60      2133\n",
      "\n",
      "Accuracy: 0.508\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Results for stupid classifier:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       214\n",
      "          1       0.90      1.00      0.95      1919\n",
      "\n",
      "avg / total       0.81      0.90      0.85      2133\n",
      "\n",
      "Accuracy: 0.900\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred_skewed_random = predict_random(y_test_skewed)\n",
    "y_pred_skewed_stupid = predict_stupid(y_test_skewed)\n",
    "    \n",
    "print(\"Results for random classifier:\")\n",
    "print(classification_report(y_test_skewed, y_pred_skewed_random))\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy_score(y_test_skewed, y_pred_skewed_random)))\n",
    "print()\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print()\n",
    "print(\"Results for stupid classifier:\")\n",
    "print(classification_report(y_test_skewed, y_pred_skewed_stupid))\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy_score(y_test_skewed, y_pred_skewed_stupid)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important result is that the stupid classifier has a good accuracy since it is right most of the time due to the skewed dataset. Just looking at the accuracy can lead to wrong conclusions. In realty, most datasets are not perfectly balanced -- at least if they are not carefully handcrafted. Also the average scores look pretty good, with an f1-score of 0.71 (compared to the 0.79 if the true classifier). Only the individual results for each class show the real picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
