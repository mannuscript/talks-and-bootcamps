{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. TF and TF-idf Solution.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gpPUP-KhbgaB","colab_type":"text"},"source":["# 2nd Exercise\n","\n","Prepared by: **Hardian Lawi**"]},{"cell_type":"code","metadata":{"id":"M08ry689mlF7","colab_type":"code","colab":{}},"source":["import re\n","import numpy as np\n","import pandas as pd\n","from collections import Counter\n","from sklearn.linear_model import LogisticRegression"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lxYIeaRrdtnb","colab_type":"text"},"source":["# Implement Tf-Idf manually\n","\n","In this section, we will implement Tf-Idf method manually to better understand the process. Also, instead of using huge public dataset, here we will start by using a small corpus to check our implementation."]},{"cell_type":"code","metadata":{"id":"geAIBkCiiv0r","colab_type":"code","colab":{}},"source":["corpus = [\n","    'This is the first document.',\n","    'This document is the second document.',\n","    'And this is the third one.',\n","    'Is this the first document?',\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O110CtDUe_in","colab_type":"text"},"source":["## Tokenize corpus"]},{"cell_type":"code","metadata":{"id":"d2LLmI93Th8n","colab_type":"code","outputId":"6c916aa0-c842-43ea-a53a-1c10e06bf607","executionInfo":{"status":"ok","timestamp":1563124093819,"user_tz":-420,"elapsed":1122,"user":{"displayName":"Hardian Lawi","photoUrl":"","userId":"05768104717203707125"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["rx = re.compile(r'\\w+')\n","tokenized_corpus = [rx.findall(x.lower()) for x in corpus]\n","tokenized_corpus"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['this', 'is', 'the', 'first', 'document'],\n"," ['this', 'document', 'is', 'the', 'second', 'document'],\n"," ['and', 'this', 'is', 'the', 'third', 'one'],\n"," ['is', 'this', 'the', 'first', 'document']]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"MaeUPQX1hAzd","colab_type":"text"},"source":["Store some useful variables for later use"]},{"cell_type":"code","metadata":{"id":"SnWh7nGtgoNj","colab_type":"code","outputId":"872b337c-209d-4ce1-ee73-ee70323a0b2a","executionInfo":{"status":"ok","timestamp":1563124094133,"user_tz":-420,"elapsed":1425,"user":{"displayName":"Hardian Lawi","photoUrl":"","userId":"05768104717203707125"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["# Store size of vocabulary and number of documents\n","vocabs = set([x for l in tokenized_corpus for x in l])\n","vocab_size = len(vocabs)\n","no_docs = len(tokenized_corpus)\n","\n","# Create a mapping between token and a unique integer\n","token2id = dict(zip(sorted(vocabs), range(vocab_size)))\n","token2id"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'and': 0,\n"," 'document': 1,\n"," 'first': 2,\n"," 'is': 3,\n"," 'one': 4,\n"," 'second': 5,\n"," 'the': 6,\n"," 'third': 7,\n"," 'this': 8}"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"D0SiRVehhF55","colab_type":"text"},"source":["## Term-frequency\n","\n","$\\text{tf}(t, d)$ is the number of times a term $t$ occurs in a given document $d$."]},{"cell_type":"code","metadata":{"id":"frLTgY_FT-A0","colab_type":"code","outputId":"af407ed3-c788-4402-ad69-cef576e03090","executionInfo":{"status":"ok","timestamp":1563124094134,"user_tz":-420,"elapsed":1420,"user":{"displayName":"Hardian Lawi","photoUrl":"","userId":"05768104717203707125"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["def generate_tf(tokenized_corpus, token2id):\n","  tf = np.zeros((len(tokenized_corpus), len(token2id)), dtype=int)\n","  for i in range(len(tokenized_corpus)):\n","    token_counts = Counter(tokenized_corpus[i])\n","    for t, c in token_counts.most_common():\n","        tf[i, token2id[t]] = c\n","  return tf\n","    \n","tf = generate_tf(tokenized_corpus, token2id)\n","tf"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n","       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n","       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n","       [0, 1, 1, 1, 0, 0, 1, 0, 1]])"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"Yr5u68yDhJc6","colab_type":"text"},"source":["## Inverse document frequency\n","\n","$$\\text{idf}(t) = \\log{\\frac{n}{\\text{df}(t)}} + 1$$\n","\n","where $n$ is the total number of documents in the document set, and $\\text{df}(t)$ is the number of documents in the document set that contain term $t$."]},{"cell_type":"code","metadata":{"id":"T4HxexOyjC6e","colab_type":"code","outputId":"cfb81d09-df76-421f-db76-bbf7a4542e11","executionInfo":{"status":"ok","timestamp":1563124094134,"user_tz":-420,"elapsed":1409,"user":{"displayName":"Hardian Lawi","photoUrl":"","userId":"05768104717203707125"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Start by computing df\n","df = np.zeros((1, vocab_size), dtype=int)\n","for i in range(no_docs):\n","  unique_tokens = set(tokenized_corpus[i])\n","  for t in unique_tokens:\n","    df[0, token2id[t]] += 1\n","    \n","df"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1, 3, 2, 4, 1, 1, 4, 1, 4]])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"ZVis6_P3iuZ1","colab_type":"code","outputId":"c2bca653-944c-4c50-9160-d173c8146d9e","executionInfo":{"status":"ok","timestamp":1563124094135,"user_tz":-420,"elapsed":1403,"user":{"displayName":"Hardian Lawi","photoUrl":"","userId":"05768104717203707125"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["idf = np.zeros((1, vocab_size), dtype=float)\n","for i in range(vocab_size):\n","  idf[0, i] = np.log(no_docs / df[0, i]) + 1\n","  \n","idf"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[2.38629436, 1.28768207, 1.69314718, 1.        , 2.38629436,\n","        2.38629436, 1.        , 2.38629436, 1.        ]])"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"37Ej_Mbpjz6_","colab_type":"text"},"source":["## Tf-Idf representation of corpus\n","\n","$$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)$$"]},{"cell_type":"code","metadata":{"id":"uGOjkptmjwUh","colab_type":"code","outputId":"9666fe01-6a71-4f78-b0d6-2acacb53da90","executionInfo":{"status":"ok","timestamp":1563124094136,"user_tz":-420,"elapsed":1394,"user":{"displayName":"Hardian Lawi","photoUrl":"","userId":"05768104717203707125"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["tfidf = tf * idf\n","tfidf"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.        , 1.28768207, 1.69314718, 1.        , 0.        ,\n","        0.        , 1.        , 0.        , 1.        ],\n","       [0.        , 2.57536414, 0.        , 1.        , 0.        ,\n","        2.38629436, 1.        , 0.        , 1.        ],\n","       [2.38629436, 0.        , 0.        , 1.        , 2.38629436,\n","        0.        , 1.        , 2.38629436, 1.        ],\n","       [0.        , 1.28768207, 1.69314718, 1.        , 0.        ,\n","        0.        , 1.        , 0.        , 1.        ]])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"IR8mgg3OkX4m","colab_type":"text"},"source":["## Final Touch\n","\n","To avoid the numbers to explode when dealing with huge corpus, we could scale the resulting vectors by Euclidean norm:\n","\n","$$v_{norm} =  \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}}$$\n","\n","where $v \\in \\mathbb{R}^d$ for some $d$ and $v_i$ is the $i$-th element of $v$."]},{"cell_type":"code","metadata":{"id":"zRCPmezmljz-","colab_type":"code","outputId":"d69f61fc-83cd-4a15-c0bc-aaa723e3c109","executionInfo":{"status":"ok","timestamp":1563124094137,"user_tz":-420,"elapsed":1386,"user":{"displayName":"Hardian Lawi","photoUrl":"","userId":"05768104717203707125"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["tfidf = tfidf / np.sqrt((tfidf ** 2).sum(axis=1, keepdims=True))\n","tfidf"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.        , 0.46941728, 0.61722732, 0.3645444 , 0.        ,\n","        0.        , 0.3645444 , 0.        , 0.3645444 ],\n","       [0.        , 0.65782665, 0.        , 0.25543054, 0.        ,\n","        0.60953246, 0.25543054, 0.        , 0.25543054],\n","       [0.53248519, 0.        , 0.        , 0.22314313, 0.53248519,\n","        0.        , 0.22314313, 0.53248519, 0.22314313],\n","       [0.        , 0.46941728, 0.61722732, 0.3645444 , 0.        ,\n","        0.        , 0.3645444 , 0.        , 0.3645444 ]])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"wJdBco7snGjP","colab_type":"text"},"source":["\n","## Verify with scikit-learn"]},{"cell_type":"code","metadata":{"id":"vqmjF59CnJlX","colab_type":"code","outputId":"94c633c0-0a1c-4a0b-f85a-446b5f3378f1","executionInfo":{"status":"ok","timestamp":1563124094139,"user_tz":-420,"elapsed":1380,"user":{"displayName":"Hardian Lawi","photoUrl":"","userId":"05768104717203707125"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer(smooth_idf=False)\n","sci_tfidf = vectorizer.fit_transform(corpus).toarray()\n","sci_tfidf"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.        , 0.46941728, 0.61722732, 0.3645444 , 0.        ,\n","        0.        , 0.3645444 , 0.        , 0.3645444 ],\n","       [0.        , 0.65782665, 0.        , 0.25543054, 0.        ,\n","        0.60953246, 0.25543054, 0.        , 0.25543054],\n","       [0.53248519, 0.        , 0.        , 0.22314313, 0.53248519,\n","        0.        , 0.22314313, 0.53248519, 0.22314313],\n","       [0.        , 0.46941728, 0.61722732, 0.3645444 , 0.        ,\n","        0.        , 0.3645444 , 0.        , 0.3645444 ]])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"CWGAmFmMn84O","colab_type":"text"},"source":["See how easy it is to apply Tf-idf using `scikit-learn`. It is always recommended to use the built-in library because it is more efficient and convenient."]},{"cell_type":"code","metadata":{"id":"e-w0tiNCplDW","colab_type":"code","colab":{}},"source":["assert np.isclose(tfidf, sci_tfidf).all()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x3JyCiidn7s1","colab_type":"text"},"source":["# Apply Tf-idf on Public Dataset"]},{"cell_type":"code","metadata":{"id":"TjqwCSnB4sym","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import CountVectorizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uY539Lg-pDUS","colab_type":"code","outputId":"7b94c2d1-ea5b-4704-a43e-f0eadddbbe07","executionInfo":{"status":"ok","timestamp":1563124101944,"user_tz":-420,"elapsed":9167,"user":{"displayName":"Hardian Lawi","photoUrl":"","userId":"05768104717203707125"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["%%bash\n","\n","wget -qO yelp_review_polarity_csv.tgz https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz\n","tar -xvzf yelp_review_polarity_csv.tgz\n","ls"],"execution_count":13,"outputs":[{"output_type":"stream","text":["yelp_review_polarity_csv/\n","yelp_review_polarity_csv/train.csv\n","yelp_review_polarity_csv/readme.txt\n","yelp_review_polarity_csv/test.csv\n","sample_data\n","yelp_review_polarity_csv\n","yelp_review_polarity_csv.tgz\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vrEuOXMrmlF-","colab_type":"code","colab":{}},"source":["train = pd.read_csv('yelp_review_polarity_csv/train.csv', names=[\"label\", \"text\"])\n","test = pd.read_csv('yelp_review_polarity_csv/test.csv', names=[\"label\", \"text\"])\n","\n","# Sample\n","train = train.head(50000)\n","test = test.head(10000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aSGq3AZZ4YbS","colab_type":"code","outputId":"d3036965-a077-49b7-877a-86f3c63126d4","executionInfo":{"status":"ok","timestamp":1563124136345,"user_tz":-420,"elapsed":43546,"user":{"displayName":"Hardian Lawi","photoUrl":"","userId":"05768104717203707125"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["vectorizer = CountVectorizer()\n","\n","count_X_train, y_train = vectorizer.fit_transform(train.text), train.label - 1\n","count_X_test, y_test = vectorizer.transform(test.text), test.label - 1\n","\n","print('train size:', count_X_train.shape)\n","print('test size:', count_X_test.shape)\n","\n","model = LogisticRegression()\n","model = model.fit(count_X_train, y_train)\n","\n","print('\\nCount Representation performance')\n","print('Train acc:', (model.predict(count_X_train) == y_train).mean())\n","print('Test acc:', (model.predict(count_X_test) == y_test).mean())"],"execution_count":15,"outputs":[{"output_type":"stream","text":["train size: (50000, 60652)\n","test size: (10000, 60652)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Count Representation performance\n","Train acc: 0.98464\n","Test acc: 0.9205\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7ipQuhrY5MwB","colab_type":"code","colab":{}},"source":["del count_X_train, count_X_test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zbjxLO8FmlGE","colab_type":"code","outputId":"62a3a647-c790-474f-96c6-c995b8445841","executionInfo":{"status":"ok","timestamp":1563124145059,"user_tz":-420,"elapsed":52245,"user":{"displayName":"Hardian Lawi","photoUrl":"","userId":"05768104717203707125"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["vectorizer = TfidfVectorizer()\n","\n","tfidf_X_train, y_train = vectorizer.fit_transform(train.text), train.label - 1\n","tfidf_X_test, y_test = vectorizer.transform(test.text), test.label - 1\n","\n","print('train size:', tfidf_X_train.shape)\n","print('test size:', tfidf_X_test.shape)\n","\n","model = LogisticRegression()\n","model = model.fit(tfidf_X_train, y_train)\n","\n","print('\\nTF Idf Representation performance')\n","print('Train acc:', (model.predict(tfidf_X_train) == y_train).mean())\n","print('Test acc:', (model.predict(tfidf_X_test) == y_test).mean())"],"execution_count":17,"outputs":[{"output_type":"stream","text":["train size: (50000, 60652)\n","test size: (10000, 60652)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["\n","TF Idf Representation performance\n","Train acc: 0.94094\n","Test acc: 0.9202\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tIaMsHdDbY_3","colab_type":"text"},"source":["# Bonus"]},{"cell_type":"code","metadata":{"id":"q_aK1nCNZ6C1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"241d198d-92a6-43f6-b10d-8650c809d2cd","executionInfo":{"status":"ok","timestamp":1563124145060,"user_tz":-420,"elapsed":52239,"user":{"displayName":"Hardian Lawi","photoUrl":"","userId":"05768104717203707125"}}},"source":["id2token = dict(zip(vectorizer.vocabulary_.values(), vectorizer.vocabulary_.keys()))\n","\n","for i in np.argsort(model.coef_)[0][-10:]:\n","  print(id2token[i])"],"execution_count":18,"outputs":[{"output_type":"stream","text":["good\n","perfect\n","fantastic\n","excellent\n","awesome\n","best\n","love\n","amazing\n","delicious\n","great\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jpvVJSEAcXRH","colab_type":"text"},"source":["As explained, the model here performs better than word embeddings because the feature representation is suitable for the task (Sentiment Prediction). To be more concrete, when the reviews are positive, the document will usually contain positive words such as above. Since both TF and TF-idf both use each unique token as a feature, our model could easily learn this relationship and give high positive weights to all positive words."]}]}