{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis (supervised, feature-based)\n",
    "\n",
    "This tutorial shows sentiment analysis using feature extraction and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all important packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# The next imports are only needed for the preprocessing\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from utils.nlputil import preprocess_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need those for the preprocessing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load traing data\n",
    "\n",
    "As usual, we use `pandas` to read files. In this case, we have a file that conains 703 tweets together with a polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>senti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@united UA5396 can wait for me. I'm on the gro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I hate Time Warner! Soooo wish I had Vios. Can...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tom Shanahan's latest column on SDSU and its N...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Found the self driving car!! /IWo3QSvdu2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@united arrived in YYZ to take our flight to T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  senti\n",
       "0  @united UA5396 can wait for me. I'm on the gro...      0\n",
       "1  I hate Time Warner! Soooo wish I had Vios. Can...      0\n",
       "2  Tom Shanahan's latest column on SDSU and its N...      2\n",
       "3           Found the self driving car!! /IWo3QSvdu2      2\n",
       "4  @united arrived in YYZ to take our flight to T...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_train = pd.read_csv('data/twitter-sentiment/twitter-sentiment-bowden-training.csv')\n",
    "\n",
    "# Print the first 5 lines\n",
    "df_tweets_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this datset, the values for \"senti\", i.e., the labels are:\n",
    "- 0 means negative\n",
    "- 2 means neutral\n",
    "- 4 means positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `pandas` data frame, we generate two list, one containing the tweets, the other containg the polarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarities_train = df_tweets_train['senti'].tolist() \n",
    "tweets_train = df_tweets_train['tweet'].tolist() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess tweets\n",
    "\n",
    "We use our `preprocess_text()` method to preprocess all tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets_train = [''] * len(tweets_train)\n",
    "\n",
    "for idx, doc in enumerate(tweets_train):\n",
    "    processed_tweets_train[idx] = preprocess_text(doc, tokenizer=tweet_tokenizer, lemmatizer=wordnet_lemmatizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate feature set\n",
    "\n",
    "We use the `TfidfVectorizer` to calculate the document word matrix which will be our feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(processed_tweets_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of classifier\n",
    "\n",
    "Packages like `scikit-learn` make it extremely easy to train machine learning classifiers. `scikit-learn` offers a variety of classifier algorithms. Here, we use `MultinomialNB`, a Multinomial Naive Bayes classifier which is often a good choice in case of text documents. \n",
    "\n",
    "But feel free to try other classifiers! \n",
    "\n",
    "Very conveniently, all `scikit-learn` classifiers use the same methods which makes replacinf classifiers very quick and easy. For the full list of supported classifiers, see the `sklearn-learn` website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, polarities_train)\n",
    "#clf = DecisionTreeClassifier().fit(X_train_tfidf, polarities_train)\n",
    "#clf = LinearSVC().fit(X_train_tfidf, polarities_train)\n",
    "#clf = KNeighborsClassifier().fit(X_train_tfidf, polarities_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...yes, it really is just 1 line of code!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first check the classifier with 2 sample documents to see the 2 basic required steps:\n",
    "\n",
    "- convert documents into the document word matrix; note that we have to make sure that the matrix (with respect to the vocabulary) with the one generated from the training data\n",
    "\n",
    "- run classifier over the document word matrix (i.e., the feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soccer is so much fun => 4\n",
      "being hungry is shit => 0\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['soccer is so much fun', 'being hungry is shit']\n",
    "\n",
    "# Use the fitted vectorizer to transform the documents: transform() not fit_transform()!\n",
    "X_new_tfidf = tfidf_vectorizer.transform(docs_new)\n",
    "\n",
    "# Use the trained classifier to predict the polarities\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('{} => {}'.format(doc, category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data\n",
    "\n",
    "We perform the exact same steps as we did for the training data:\n",
    "\n",
    "- load the file with the test data\n",
    "\n",
    "- convert `pandas` data frame into two lists (tweets + polarities)\n",
    "\n",
    "- preprocess tweets (the same way we did the tweets of the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_test = pd.read_csv('data/twitter-sentiment/twitter-sentiment-bowden-test.csv')\n",
    "\n",
    "polarities_test = df_tweets_test['senti'].tolist() \n",
    "tweets_test = df_tweets_test['tweet'].tolist() \n",
    "\n",
    "processed_tweets_test = [''] * len(tweets_test)\n",
    "\n",
    "for idx, doc in enumerate(tweets_test):\n",
    "    processed_tweets_test[idx] = preprocess_text(doc, tokenizer=tweet_tokenizer, lemmatizer=wordnet_lemmatizer)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform test\n",
    "\n",
    "The last two steps are as outline above:\n",
    "\n",
    "- convert documents into the document word matrix; note that we have to make sure that the matrix (with respect to the vocabulary) with the one generated from the training data\n",
    "\n",
    "- run classifier over the document word matrix (i.e., the feature set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the fitted vectorizer to transform the documents: transform() not fit_transform()!\n",
    "X_test_tfidf = tfidf_vectorizer.transform(tweets_test)\n",
    "\n",
    "predicted = clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results\n",
    "\n",
    "`scikit-learn` provides a series of methods calculate a variety of metrics, particularly the precision, recall, and f1-score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[82  1 30]\n",
      " [21 14 36]\n",
      " [22  1 91]]\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.73      0.69       113\n",
      "          2       0.88      0.20      0.32        71\n",
      "          4       0.58      0.80      0.67       114\n",
      "\n",
      "avg / total       0.68      0.63      0.59       298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(polarities_test, predicted))\n",
    "print()\n",
    "print(classification_report(polarities_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use additional features (optional)\n",
    "\n",
    "This example shall illustrate that a feature vector can contain all kinds of dimensions/features. So far we only used the feature vectorss derived from the words. In the following, add to this feature vector two more values which we get from from the unsupervised sentiment analysis to boost the classifier.\n",
    "\n",
    "Remember, for the unsupervised sentiment analysis, we assinged it word with a positive or negative numerical value, and summed up all values within document to get the final sentiment values. We can use this information to add to the feature vector. Since the values must be positive, we add 2 dimensions to the vector: one indicating that the sentiment score was positive and one indicating that score was negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised sentiment analysis step\n",
    "\n",
    "We copy the folling part from the tutorial for the unsupervised sentiment analysis:\n",
    "\n",
    "- load sentiment lexicon\n",
    "\n",
    "- define method `calc_polarity` that calculates the polarity of a document depending on the sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = pd.read_csv('data/sentilex-vader.txt', sep='\\t', encoding = \"ISO-8859-1\", header=None)\n",
    "\n",
    "sentiment_dict = {}\n",
    "\n",
    "for index, row in df_sentiment.iterrows():\n",
    "    token, score = row[0], row[1]\n",
    "    sentiment_dict[token] = score / 4.0 # normalize score from [-4,...,4] to [-1,...,1]\n",
    "\n",
    "def calc_polarity(doc, num_polarities=3):\n",
    "    doc_score = 0.0\n",
    "    for token in doc.split(): # Here split() is sufficient\n",
    "        if token in sentiment_dict:\n",
    "            doc_score += sentiment_dict[token]\n",
    "    if doc_score > 0:\n",
    "        return 1.0\n",
    "    elif doc_score < 0:\n",
    "        return -1.0\n",
    "    else:\n",
    "        if num_polarities == 3:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return 1.0\n",
    "    return 0.0 # Just to be sure, should never be reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "\n",
    "The next lines cover these 3 main steps:\n",
    "\n",
    "- Calculate the basic feature set using the `TfidfVectorizer`, same as above\n",
    "\n",
    "- Calculate the extended feature set with 2 dimension for each feature vector\n",
    "\n",
    "- Merge both feature sets into one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(699, 2677)\n",
      "(699, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdw/anaconda3/lib/python3.5/site-packages/scipy/sparse/compressed.py:774: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(699, 2679)\n",
      "699\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(processed_tweets_train)\n",
    "\n",
    "print (type(X_train_tfidf))\n",
    "print (X_train_tfidf.shape)\n",
    "\n",
    "new_feature_col_train = csr_matrix((X_train_tfidf.shape[0], 2), dtype=float)\n",
    "\n",
    "print (new_feature_col_train.shape)\n",
    "\n",
    "for idx, doc in enumerate(tweets_train):\n",
    "    polarity = calc_polarity(doc)\n",
    "    if polarity > 0:\n",
    "        new_feature_col_train[idx,0] = 1\n",
    "    elif polarity < 0:\n",
    "        new_feature_col_train[idx,1] = 1  \n",
    "    #new_feature_col_train[idx] = ((calc_polarity(doc) + 1.0) / 2.0)\n",
    "\n",
    "# Merge the 699x2677 matrix with the 699x2 matrix\n",
    "X_train_tfidf = hstack((X_train_tfidf, new_feature_col_train))\n",
    "\n",
    "print (X_train_tfidf.shape)\n",
    "print (len(polarities_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating classifier\n",
    "\n",
    "From here on, everything is the same as above, only that the features set of the test data needs also be extended, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, polarities_train)\n",
    "#clf = DecisionTreeClassifier().fit(X_train_tfidf, polarities_train)\n",
    "#clf = LinearSVC().fit(X_train_tfidf, polarities_train)\n",
    "#clf = KNeighborsClassifier().fit(X_train_tfidf, polarities_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(298, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdw/anaconda3/lib/python3.5/site-packages/scipy/sparse/compressed.py:774: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(tweets_test)\n",
    "\n",
    "new_feature_col_test = csr_matrix((X_test_tfidf.shape[0], 2), dtype=float)\n",
    "\n",
    "for idx, doc in enumerate(tweets_test):\n",
    "    polarity = calc_polarity(doc)\n",
    "    if polarity > 0:\n",
    "        new_feature_col_test[idx,0] = 1\n",
    "    elif polarity < 0:\n",
    "        new_feature_col_test[idx,1] = 1  \n",
    "    #new_feature_col_train[idx] = ((calc_polarity(doc) + 1.0) / 2.0)\n",
    "\n",
    "print (new_feature_col_test.shape)\n",
    "\n",
    "X_test_tfidf = hstack((X_test_tfidf, new_feature_col_test))\n",
    "\n",
    "predicted = clf.predict(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 78   1  34]\n",
      " [ 18  18  35]\n",
      " [ 10   1 103]]\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.69      0.71       113\n",
      "          2       0.90      0.25      0.40        71\n",
      "          4       0.60      0.90      0.72       114\n",
      "\n",
      "avg / total       0.72      0.67      0.64       298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(polarities_test, predicted))\n",
    "print()\n",
    "print(classification_report(polarities_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
